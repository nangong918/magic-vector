**设计思路**
============

## 可持续升级性

AI是在不断发展的, 整个人格设计是需要更新迭代的.需要做到可持续升级
lv1: 数据结构更变
lv2: 功能升级
lv3: 架构改变

## 基本功能

App创建聊天Agent, 做出特殊设定, 包括AI记忆, Agent可以注入到嵌入式设备 (MCP)
嵌入式设备可以进行本地函数调用, 如: 移动, 视觉, 传感, 询问.
设备本身有基础Agent, 包括上述函数调用的功能, 是自发的, 用户不可控的
语音转文字stt功能
AI文本转语音tts功能
部署服务器远程控制(Iot)
Spring, Android, Esp32 三端长连接互通控制

### 已有功能研究 + SDK开发

#### 文本生成

* 流式输出
* 多轮对话
* 长上下文: Qwen-Long
* 角色扮演: Qwen-Character

#### 多模态

* 视觉理解: Qwen3-VL-Plus
  * 输入是图片/视频文件
* 实时多模态
  * 音频流式输入：Qwen-Omni 模型只能接收音频文件作为输入，而 Qwen-Omni 实时模型可以实时接收音频流；
  * 语音活动检测：Qwen-Omni 实时模型内置 VAD（Voice Activity Detection，语音活动检测）功能，可自动检测用户语音的开始和结束；
  * *当前问题：实时多模态（Qwen-Omni-Realtime）的VAD功能会打断自己说话*
    * 解决方案：
      1. 音频相似度比对思路：使用梅尔频率倒谱系数(MFCC)
      2. 使用3D-Speaker声纹识别 (需要创建Python后端)

##### 语音识别 / 合成

* 实时语音识别
* 语音合成

#### 向量化

* 文本与多模态向量化
  * 向量化模型能够将文本、图像和视频数据转化为数值向量，并用于后续的语义搜索、推荐、聚类、分类、异常检测等任务.
  * 模型根据用户的输入生成连续向量，这些输入可以是文本、图片或视频。适用于视频分类、图像分类、图文检索，以文/图搜图，以文/图搜视频等任务场景。

#### 工具调用

* 联网搜索
* Function Call
* MCP

**先实现最简单的功能, 然后在进行后续升级**

#### 计算机理论学习

SDK开发中回遇到各种协议, 需要学习好计算机网络部分
在lv0完成之后, 先系统性的学习计算机理论, Spring, Android, 然后再进行lv1的开发

## 功能详情

### Agent模块

* 创建Agent

  * Android：
    * 设置Agent名称，Agent系统级别描述设定，设置Agent头像，提交Agent创建
    * （后续升级：获取并设置Agent声音模型(tts), 设置Agent声音模型）
  * Spring：
    * 创建Agent: file文件存储minio，设定存储mysql
    * 创建gateway，对file urls进行反向代理
    * 后续升级: 接入tts模型，提供tts模型列表
* 获取最近聊天的Agent List

  * Android：
    * init：初次、再次长连接之后进行Http请求服务器，获取最新的chatList
    * 其余时候：从Room获取数据，从MMKV获取json对象数据
  * Spring：
    * init：MessageList，切面是mapper层，执行前后执行Redis查询和缓存。([lv2])
* 获取跟Agent的最近的聊天记录

  * Android
    * init：初次、再次长连接之后进行Http请求服务器，获取最新的chatList
    * sse / websocket
  * Spring：
    * init：AOP获取ChatList，切面是mapper层，执行前后执行Redis查询和缓存。([lv2])

### Chat模块

* 选择 / 创建 session 进行聊天
  * Android：
    * websocket长连接
    * VAD
  * Spring：
    * 获取最近的ChatList组成ChatMemory + 当前消息交给AI模型，返回结果给前端
    * Mysql分表存储Chat记录 (后续升级, 目前最重要的是跑通核心代码[lv2])
    * VAD + 声纹识别[lv1]

### 实时语音通话模块

* 调用实时多模态(Qwen-Omni-Realtime)
* VAD: 虽然Qwen-Omni内置VAD, 但是Android仍实现VAD, 避免频繁调用接口(集成JNI调用WebRTC_vad[lv1])
* 使用WebSocket而不是WebRTC: 非真正的语音通话, 无需集成WebRTC技术
* 非人声降噪: FFmpeg高级滤波器 (需要集成FFmpeg[lv2])
* 音频压缩: MediaCodec([lv1])
* 实时视频理解([lv2])

### 嵌入式模块

* 实时多模态Chat：
  * esp32-sc：
    * 流媒体传输实现
    * VAD
    * 声纹识别

* 表情模块
  * esp32-sc：
    * OLED屏幕
    * 情感系统

* 移动功能
  * esp32-sc：
    * s90舵机，移动函数编写
    * mqtt长连接 + function call
  * Spring：
    * Mqtt长连接
    * Mcp给ChatModel调用的Function Call

### 工具调用

* 联网搜索MCP([lv1])
* Function Call
* 三方MCP调用([lv2])

### 人格模块([lv1])

#### 人格模块思路

人格系统的很复杂的操作系统, 需要好好的理清设计思路, 当然也期待未来开源的人格操作系统

人格注入: App的人格创建分为人类人格 \ 机器狗人格, 只有机器狗人格才能注入到嵌入设设备. 如果人类人格注入到嵌入式设备要主动的展示不适应.

* 设立基本人设, 系统级别人设
* AI在交互中识别事件, 创建: **事件-想法** 对象, 精炼想法并存入 人格库 (提示词库)
* 创建RAG人格库向量索引
* 设立人格操作系统, 根据分值去指示AI想要主动做什么, 将各个方向的想法数值打平为向量, 匹配想要做的想法 (问题: 人类是复杂多线程的)
* 创建想法的时候最好要有: 历史事件, 操作系统数值 \ 想法向量支撑
* 学习 + 好奇行为: 主动获取身边的环境和信息, 将信息存储到知识库中; 联想: 与知识库中的信息进行匹配联想.

### 躯体模块

ESP32-SC; sg90舵机, LED表情屏幕, ESP32-CAM摄像头, 录音 + 播放器, 电磁感应充电 + 电磁感应充电桩

* 表情行为
* 移动行为
* 红外感知 + 摄像头行为([lv1])
* 音频感知 + 播放器行为
* 电量感知 + 充电行为([lv2])

## 整体架构

### Spring设计

* 数据库
  PostgreSQL(TimeScaleDB), Neo4j, ElasticSearch, Milvus
* 缓存
  Redis
* 网络请求

WebFlux

* 长连接

前端长连接: Netty

嵌入式长连接: MQTT

* 异步

JDK21虚拟线程 (避免线程池 + 分布式)

* Oss存储

minio

* 语音通话: WebRTC(内部自带VAD)
* 音频处理
  stt和tts: 阿里百炼
* 物联网监控
  暂选WebRTC; 升级选择RTMP
  视频编码调整 -> 视频的效果

#### AI设计

使用SpringAI框架, 快速集成ChatModel

* 联网功能
  集成`阿里百炼`的访问互联网MCP
* 聊天记忆
  意图 + 实体识别 -> 成功识别 -> 结构化匹配.
  识别失败 -> 向量索引.

短期记忆存储Redis \ ChatMemory
长期记忆存储数据库

### 客户端设计

Jetpack Compose Android

* 界面
  Jetpack Compose混合XML
* 长连接 (Todo: 列出为什么使用Netty和Mqtt)

后端长连接: Netty

嵌入式端长连接: MQTT (IOT) 远程唤醒, 视频监控

* 设计模式

Mvvm设计模式, LiveData更新View

* 异步

Kotlin 协程

* 缓存

MMKV\SharePrefence -> View数据

Room(SQLite) -> 聊天记录

* 网络请求

TCP\IP
流式Http传输

OKhttp\Retrofit

* 语音通话
  WebRTC
* 物联网视频监控
  暂时选用WebRTC, 升级选择RTMP

### 嵌入式设计

* 开发板

ESP32-SC

* 屏幕
* 音频录制\发出
* 运动舵机
* 传感器
* 摄像头

### 服务部署

* Docker
* Kubernetes(K8s)
* 阿里云服务器
* Nginx反向代理域名和minio内部资源

### 注意

开发的时候需要先进行App混淆和Spring部署测试

### 开发测试

Android反编译与混淆

JMeter压测与内存监控
